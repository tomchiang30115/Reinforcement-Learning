{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0878949",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Imports #\n",
    "###########\n",
    "\n",
    "# Mario Envrionment Libraries\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "# Other Libraries\n",
    "import gym\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Agent Class #\n",
    "###############\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_actions, model_path=None):\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = 0.0\n",
    "        \n",
    "        # Allow torch to use GPU for processing, if avaiable\n",
    "        device = torch.device('cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        print(\"Torch device: \", device)\n",
    "        \n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        \n",
    "        self.q1.to(device)\n",
    "        \n",
    "        # Check if previously trained model has been provided\n",
    "        if model_path != None:\n",
    "            loaded_model = torch.load(model_path)\n",
    "            self.q1.load_state_dict(loaded_model['model_state_dict'])\n",
    "            self.q1.eval()\n",
    "            self.epsilon = loaded_model['epsilon']\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        # epsilon-greedy policy\n",
    "        if (random.uniform(0, 1) < self.epsilon):\n",
    "            # Explore randomly\n",
    "            random_action = random.randint(0, self.num_actions-1)\n",
    "            #print(\"Selected action:\", random_action)\n",
    "            return random_action\n",
    "        else:\n",
    "            # Follow policy greedily (A_t = argmax_q1(S_t, a, theta1))\n",
    "            state = torch.tensor(state.__array__()).squeeze().cuda() # Squeeze to get rid of uneccessary channel dimension\n",
    "            #print(state.shape)\n",
    "            state = torchvision.transforms.functional.convert_image_dtype(state, dtype=torch.float)\n",
    "            state = state.unsqueeze(0) # Add first dimension to match shape of minibatches being fed into network\n",
    "            #print(state.shape)\n",
    "            \n",
    "            action_values = self.q1(state)\n",
    "            #print(action_values)\n",
    "            best_action = torch.argmax(action_values, axis=1).item()\n",
    "            #print(\"Selected action:\", best_action)\n",
    "            return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrameSkipReplicator class modified from the original MaxAndSkipEnv wrapper to display all 4 frames (for nice video playback), but only return max_frame\n",
    "from stable_baselines3.common.type_aliases import GymStepReturn\n",
    "class FrameSkipReplicator(MaxAndSkipEnv):\n",
    "    def __init__(self, env, skip=4, fps=60):\n",
    "        super().__init__(env, skip)\n",
    "        self.fps = fps\n",
    "        self.skip = skip\n",
    "    def step(self, action: int) -> GymStepReturn:\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        last_frame_rendered = time.perf_counter()\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            current_time = time.perf_counter()\n",
    "            while current_time - last_frame_rendered < 1/self.fps:\n",
    "                current_time = time.perf_counter()\n",
    "            last_frame_rendered = current_time\n",
    "            env.render()\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Playback loop to see trained agent performance #\n",
    "##################################################\n",
    "\n",
    "# Seed the RNG for consistent sequence of random actions\n",
    "random.seed(1)\n",
    "\n",
    "# Setup a new environment without frameskipping (so that playback looks normal)\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "\n",
    "# Simplest moveset that can complete the level\n",
    "NO_RUNNING = [\n",
    "    ['right'],\n",
    "    ['right', 'A']\n",
    "]\n",
    "\n",
    "# Specify what controller inputs are available\n",
    "chosen_inputs = SIMPLE_MOVEMENT\n",
    "env = JoypadSpace(env, chosen_inputs)\n",
    "\n",
    "# Wrapper to ensure observations are in same format as was used in training\n",
    "num_skip_frames = 4\n",
    "game_fps = 60\n",
    "env = FrameSkipReplicator(env, skip=num_skip_frames, fps=game_fps) # Replicate the frameskipping progress (one ouptut every n frames), but render all skipped frames\n",
    "env = GrayScaleObservation(env) # Convert state from RGB to grayscale (1/3 number of pixels for NN to process)\n",
    "env = ResizeObservation(env, shape=84) # Scale each frame to 64x64 pixels (15x fewer pixels for NN to process)\n",
    "env = FrameStack(env, 4) # Stack the last 4 observations together to give NN temporal awareness\n",
    "\n",
    "# Create a new agent using saved model\n",
    "agent = Agent(len(chosen_inputs), model_path='./models/lr0.000025/20220419-122232/10000000.pth')\n",
    "#agent.epsilon = 0.05 # Use this if you still want some degree of randomness in playback or to match training value\n",
    "print(agent.epsilon)\n",
    "\n",
    "steps_since_restart = 0\n",
    "last_frame_rendered = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    #env.render()\n",
    "    #time.sleep(5)\n",
    "    for episode in range(100):\n",
    "        state = env.reset()\n",
    "        start_time = time.perf_counter()\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                chosen_action = agent.get_action(state)\n",
    "                # Perform n frames of the same action to account for the frameskipping used in training\n",
    "                state, reward, done, info = env.step(chosen_action)\n",
    "\n",
    "                #plt.imshow(state, cmap='gray')\n",
    "\n",
    "                steps_since_restart += 1\n",
    "\n",
    "                if done:\n",
    "                    print(\"Episode\", episode, end=': ')\n",
    "                    if info['time'] == 0:\n",
    "                        print(\"Mario ran out of time\")\n",
    "                    elif info['flag_get']:\n",
    "                        print(\"########## Mario reached the FLAG in\", time.perf_counter() - start_time, \"seconds ##########\")\n",
    "                    else:\n",
    "                        print(\"Mario died after\", steps_since_restart, \"steps at x_position\", info['x_pos'])\n",
    "                    break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf9967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
