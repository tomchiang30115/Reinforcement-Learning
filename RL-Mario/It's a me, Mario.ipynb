{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Imports #\n",
    "###########\n",
    "\n",
    "# Mario Envrionment Libraries\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "# Other Libraries\n",
    "import gym\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Agent Class #\n",
    "###############\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_actions, replay_buffer_size=100000, num_replay_samples=32, model_path=None):\n",
    "        self.num_actions = num_actions\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size) # Stores up to the given number of past experiences (called 'D' in slides)\n",
    "        self.num_replay_samples = num_replay_samples\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99999975\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.9\n",
    "        self.loss = torch.nn.HuberLoss()\n",
    "        self.learning_rate = 0.000025\n",
    "        \n",
    "        # Allow torch to use GPU for processing, if avaiable\n",
    "        device = torch.device('cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        print(\"Torch device: \", device)\n",
    "        \n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.previous_episodes = 0\n",
    "        self.previous_steps = 0\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.q1.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Check if previously trained model has been provided\n",
    "        if model_path != None:\n",
    "            loaded_model = torch.load(model_path)\n",
    "            self.q1.load_state_dict(loaded_model['model_state_dict'])\n",
    "            self.q1.train()\n",
    "            self.optimizer = torch.optim.Adam(self.q1.parameters(), lr=loaded_model['learning_rate'])\n",
    "            self.optimizer.load_state_dict(loaded_model['optimizer_state_dict'])\n",
    "            self.previous_episodes = loaded_model['num_episodes']\n",
    "            self.previous_steps = loaded_model['num_steps']\n",
    "            self.epsilon = loaded_model['epsilon']\n",
    "            \n",
    "        self.q2 = copy.deepcopy(self.q1) # Create target action-value network as copy of action-value network q1\n",
    "        \n",
    "        # Prevent weights being updated in target network\n",
    "        for param in self.q2.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "    \n",
    "    def update_logs(self, save_location, log_interval, episode, step, average_episode_loss, \n",
    "                    average_episode_reward, average_episode_distance, flag_count, death_count, timeout_count):\n",
    "        print(\"Updating logs\")\n",
    "        self.log_file = open(save_location + 'log.txt', 'a')\n",
    "        self.log_file.write(str(episode+self.previous_episodes) + \",\" \n",
    "                            + str(step+self.previous_steps) + \",\" \n",
    "                            + str(average_episode_reward) + \",\"\n",
    "                            + str(average_episode_distance) + \",\"\n",
    "                            + str(average_episode_loss) + \",\"\n",
    "                            + str(flag_count) + \",\"\n",
    "                            + str(death_count) + \",\"\n",
    "                            + str(timeout_count) + \",\"\n",
    "                            + str(self.epsilon) + \"\\n\")\n",
    "        self.log_file.close()\n",
    "    \n",
    "    def save_model(self, step, episode, save_location):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.q1.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': self.loss,\n",
    "            'num_episodes': episode + self.previous_episodes,\n",
    "            'num_steps': step + self.previous_steps,\n",
    "            'epsilon': self.epsilon,\n",
    "            'learning_rate': self.learning_rate\n",
    "        },\n",
    "            save_location + str(step + self.previous_steps) + '.pth')\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        # epsilon-greedy policy\n",
    "        if (random.uniform(0, 1) < self.epsilon):\n",
    "            # Explore randomly\n",
    "            random_action = random.randint(0, self.num_actions-1)\n",
    "            #print(\"Selected action:\", random_action)\n",
    "            return random_action\n",
    "        else:\n",
    "            # Follow policy greedily (A_t = argmax_q1(S_t, a, theta1))\n",
    "            state = torch.tensor(state.__array__()).squeeze().cuda() # Squeeze to get rid of uneccessary channel dimension\n",
    "            #print(state.shape)\n",
    "            state = torchvision.transforms.functional.convert_image_dtype(state, dtype=torch.float)\n",
    "            state = state.unsqueeze(0) # Add first dimension to match shape of minibatches being fed into network\n",
    "            #print(state.shape)\n",
    "            \n",
    "            action_values = self.q1(state)\n",
    "            #print(action_values)\n",
    "            best_action = torch.argmax(action_values, axis=1).item()\n",
    "            #print(\"Selected action:\", best_action)\n",
    "            return best_action\n",
    "        \n",
    "    def add_to_replay_buffer(self, state, chosen_action, reward, next_state, done):\n",
    "        # Convert to tensors\n",
    "        state = torch.tensor(state.__array__()).squeeze().cuda()\n",
    "        chosen_action = torch.tensor([chosen_action]).cuda()\n",
    "        reward = torch.tensor([reward]).cuda()\n",
    "        next_state = torch.tensor(next_state.__array__()).squeeze().cuda()\n",
    "        done = torch.tensor([int(done)]).cuda()\n",
    "        \n",
    "        self.replay_buffer.append((state, chosen_action, reward, next_state, done))\n",
    "        \n",
    "    def sync_networks(self):\n",
    "        self.q2.load_state_dict(self.q1.state_dict())\n",
    "        \n",
    "    def perform_updates(self):\n",
    "        num_samples = self.num_replay_samples\n",
    "        \n",
    "        # Check if replay buffer has enough samples for full minibatch\n",
    "        # Don't perform updates until enough samples in replay buffer\n",
    "        if len(self.replay_buffer) < self.num_replay_samples:\n",
    "            #print(\"Replay buffer only has\", len(self.replay_buffer), \"transitions. Skipping updates...\")\n",
    "            return\n",
    "        \n",
    "        # Randomly select a number of transitions from the replay buffer\n",
    "        minibatch = random.sample(self.replay_buffer, self.num_replay_samples)\n",
    "        state, chosen_action, reward, next_state, done = map(torch.stack, zip(*minibatch)) # https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html?highlight=transformer\n",
    "        chosen_action = chosen_action.squeeze()\n",
    "        reward = reward.squeeze()\n",
    "        done = done.squeeze()\n",
    "        #print(\"Shape of batch:\", next_state.shape)\n",
    "        \n",
    "        # Get TD Value Estimate\n",
    "        state = torchvision.transforms.functional.convert_image_dtype(state, dtype=torch.float)\n",
    "        estimated_value = self.q1(state)[np.arange(0, self.num_replay_samples), chosen_action]\n",
    "        \n",
    "        # Get TD Value Target\n",
    "        with torch.no_grad(): # Disable gradient calculation: https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
    "            next_state = torchvision.transforms.functional.convert_image_dtype(next_state, dtype=torch.float)\n",
    "            next_state_Q = self.q1(next_state)\n",
    "            best_action = torch.argmax(next_state_Q, axis=1)\n",
    "            next_Q = self.q2(next_state)[np.arange(0, self.num_replay_samples), best_action]\n",
    "\n",
    "        target_value = (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "        # print(\"TD Target:\", target_value)\n",
    "        \n",
    "        # Calculate loss using Huber Loss\n",
    "        loss = self.loss(estimated_value, target_value)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Stuff for logging\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Training Environment Setup #\n",
    "##############################\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "chosen_inputs = SIMPLE_MOVEMENT\n",
    "env = JoypadSpace(env, chosen_inputs)\n",
    "\n",
    "# Environment Preprocessing - Apply wrappers to the environment to reduce load on NN\n",
    "num_skip_frames = 4\n",
    "env = MaxAndSkipEnv(env, skip=num_skip_frames) # Skip the given number of frames\n",
    "env = GrayScaleObservation(env) # Convert state from RGB to grayscale (1/3 number of pixels for NN to process)\n",
    "env = ResizeObservation(env, shape=84) # Scale each frame to 64x64 pixels (15x fewer pixels for NN to process)\n",
    "env = FrameStack(env, 4) # Stack the last 4 observations together to give NN temporal awareness\n",
    "# Final observation shape: (        4         ,   84   ,   84    ,      1      )\n",
    "#                           num_stacked_frames  height    width    num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ce54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Agent Training Loop #\n",
    "#######################\n",
    "\n",
    "resuming_training = False\n",
    "saved_model_path = './models/20220418-175357/30000.pth'\n",
    "\n",
    "if resuming_training:\n",
    "    agent = Agent(len(chosen_inputs), model_path=saved_model_path) # Continue training existing model\n",
    "else:\n",
    "    agent = Agent(len(chosen_inputs)) # Train NEW agent, set resuming_training to false if training from scratch\n",
    "\n",
    "num_episodes = 1000000\n",
    "sync_interval = 10000 #(steps)\n",
    "save_interval = 10000 #(steps)\n",
    "log_interval = 20 #(episodes)\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_location = './models/lr0.000025/' + timestamp + '/'\n",
    "os.mkdir(save_location)\n",
    "\n",
    "step = 1\n",
    "rewards_per_episode = []\n",
    "distance_per_episode = []\n",
    "average_loss_per_episode = []\n",
    "death_count = 0\n",
    "timeout_count = 0\n",
    "flag_count = 0\n",
    "for episode in range(num_episodes):\n",
    "    episode_loss = 0.0\n",
    "    num_losses = 0\n",
    "    average_episode_loss = 0.0\n",
    "    episode_reward = 0.0\n",
    "    num_episode_steps = 1\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "            \n",
    "        # Choose and execute an action following epsilon-greedy policy\n",
    "        chosen_action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(chosen_action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Add the transition to the replay buffer\n",
    "        agent.add_to_replay_buffer(state, chosen_action, reward, next_state, done)\n",
    "        \n",
    "        # Update random sample of transitions from the replay buffer\n",
    "        loss = agent.perform_updates()\n",
    "        if loss is not None:\n",
    "            episode_loss += loss\n",
    "            num_losses += 1\n",
    "            average_episode_loss = episode_loss / num_losses\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        if step % 10000 == 0:\n",
    "            print(\"Done\", step, \"steps,\", episode, \"completed episodes\")\n",
    "        \n",
    "        # Sync parameters of target network every so often\n",
    "        if step % sync_interval == 0:\n",
    "            agent.sync_networks()\n",
    "            \n",
    "        # Save model every so often\n",
    "        if step % save_interval == 0:   \n",
    "            agent.save_model(step, episode, save_location)\n",
    "        \n",
    "        #env.render()\n",
    "        \n",
    "        state = next_state\n",
    "        step += 1 # Total steps over all episodes\n",
    "        num_episode_steps += 1 # Steps in this episode\n",
    "        \n",
    "        if done:\n",
    "            rewards_per_episode.append(episode_reward)\n",
    "            distance_per_episode.append(info['x_pos'])\n",
    "            average_loss_per_episode.append(average_episode_loss)\n",
    "            \n",
    "            if info['time'] == 0:\n",
    "                timeout_count += 1\n",
    "            elif info['flag_get']:\n",
    "                flag_count += 1\n",
    "            else:\n",
    "                death_count += 1\n",
    "            \n",
    "            # Log episode stats\n",
    "            if episode % log_interval == 0 and episode != 0:\n",
    "                average_episode_reward = sum(rewards_per_episode) / len(rewards_per_episode)\n",
    "                average_episode_distance = sum(distance_per_episode) / len(distance_per_episode)\n",
    "                average_loss_across_episodes = sum(average_loss_per_episode) / len(average_loss_per_episode)\n",
    "                agent.update_logs(save_location, log_interval, episode, step, average_loss_across_episodes, \n",
    "                                  average_episode_reward, average_episode_distance, flag_count, death_count, timeout_count)\n",
    "                rewards_per_episode = []\n",
    "                distance_per_episode = []\n",
    "                average_loss_per_episode = []\n",
    "                timeout_count = 0\n",
    "                flag_count = 0\n",
    "                death_count = 0\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc3f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
